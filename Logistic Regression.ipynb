{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regresión logística\n",
    "\n",
    "## Clasificación entre dos clases\n",
    "\n",
    "Fuentes:\n",
    "\n",
    "- http://www.stat.cmu.edu/~cshalizi/uADA/12/lectures/ch12.pdf\n",
    "\n",
    "Videos:\n",
    "\n",
    "https://www.youtube.com/watch?v=-la3q9d7AKQ\n",
    "https://www.youtube.com/watch?v=t1IT5hZfS48\n",
    "https://www.youtube.com/watch?v=F_VG4LNjZZw\n",
    "https://www.youtube.com/watch?v=HIQlmHxI6-0\n",
    "https://www.youtube.com/watch?v=TTdcc21Ko9A\n",
    "https://www.youtube.com/watch?v=6vO3DVJlsK4\n",
    "\n",
    "Discusión: ¿cómo encararían un problema cuya variable a predecir es categórica y no continua?¿Aplica la regresión lineal?\n",
    "\n",
    "Supongamos ahora que tenemos una variable de salida categórica cuyos valores pueden ser 0 o 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper import plot_boundaries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "x1=3\n",
    "x2=1\n",
    "x0=-2\n",
    "noise=1\n",
    "def boundary(x):\n",
    "    out=np.sign([-(x[:,0]*x1+x[:,1]*x2+x0),(x[:,0]*x1+x[:,1]*x2+x0)])/2+0.5\n",
    "    print(out)\n",
    "    return np.array(out).transpose()\n",
    "\n",
    "x_train=np.random.uniform(low=-2.5, high=2.5, size=[120,2])\n",
    "x_test=np.random.uniform(low=-2.5, high=2.5, size=[80,2])\n",
    "y_train=[np.sign(i[0]*x1+i[1]*x2+x0) for i in x_train]\n",
    "y_test=[np.sign(i[0]*x1+i[1]*x2+x0) for i in x_test]\n",
    "x_train=x_train+noise*np.random.normal(0,1,x_train.shape)\n",
    "x_test=x_test+noise*np.random.normal(0,1,x_test.shape)\n",
    "\n",
    "plot_boundaries(x_train,x_test,y_train,y_test,0,boundary)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un criterio para modelizar el dataset anterior es plantear que deacuerdo a la posición de un patrón en particular en el plano, ese patrón tiene una probabildad P de pertenecer a la clase 1 y 1-P de pertenecer a la clase 2.\n",
    "\n",
    "La idea es que la transición de la zona en la que los patrones pertenecen a la clase 1 y la zona en la que los patrones pertenecen a la clase 2, tenga una transición suave.\n",
    "\n",
    "\n",
    "Una hipótesis para nuestro problema podría ser que un patrón en particular (x1,x2) tiene una probabilidad de ser :\n",
    "\n",
    "$$ P(h) = \\frac{1}{1 + e^{-h}} $$\n",
    "\n",
    "A la función definida anteriormente se la llama sigmoide y tiene la siguiente forma.\n",
    "\n",
    "<img src=\"800px-Funci%C3%B3n_sigmoide_01.svg.png\" width=50%>\n",
    "\n",
    "Cuando h=0, habrá una zona en la cual la probabilidad de que un patrón sea de la clase 1 es 0.5. Este valor definirá el umbral entre los patrones de tipo 1 y los patrones de tipo 2.  \n",
    "\n",
    "Si queremos que para nuestro problema de clasificación dicho umbral sea una recta, podemos definir:\n",
    "\n",
    "$$ h= \\beta_0+\\beta_1*x_1+\\beta_2*x_2 $$\n",
    "\n",
    "Nótese que cuando h=0, la probabilidad de un patrón de pertenecer a la clase 1, es 0.5. Esto ocurre en el espacio geométrico:\n",
    "\n",
    "$$ x_2=-\\frac{\\beta_1}{\\beta_2}.x_1-\\frac{\\beta_0}{\\beta_2} $$\n",
    "\n",
    "Estamos proponiendo un modelo probabilístico de parámetros $\\beta_1,\\beta_2,\\beta_0$ y por lo tanto la pregunta es:\n",
    "\n",
    "** ¿Cuáles son los valores de $\\beta_1,\\beta_2,\\beta_0$ que maximizan el likelihood de este modelo?**\n",
    "\n",
    "Para ello, podemos plantear el likelihood del modelo como la probabilidad de que este modelo haya generado los patrones observados. Si suponemos que las observaciones son independientes nos queda:\n",
    "\n",
    "$$\\mathcal{L} = \\prod_{i=1}^{N} Pr(Y=y_i|X=x_i) = \\prod_{i=1}^{N} p(x_i;\\beta_1,\\beta_2,\\beta_0)^{y_i}.[1 -p(x_i;\\beta_1,\\beta_2,\\beta_0)]^{1 - y_i} = $$\n",
    "$$=\\prod_{i=1}^{N} \\left(\\frac{1}{1 + e^{-(\\beta_0+\\beta_1*x_1+\\beta_2*x_2)}}\\right)^{y_i} . \\left(1- \\frac{1}{1 + e^{-(\\beta_0+\\beta_1*x_1+\\beta_2*x_2)}}\\right)^{1-y_i} $$\n",
    "\n",
    "Queremos encontrar los parámetros $\\beta_i$ que maximicen el likelihood. Como hemos visto anteriormente, maximizar el likelihood equivale a maximizar el logaritmo de éste, ya que el logaritmo es una función monótona creciente. Una forma de hacerlo es utilizar el metodo de Gradient Descent y minimizar el $-log(\\mathcal{L})$. Al $-log(\\mathcal{L})$ se lo denomina **cross-entropy** cost function y en general está definido como:\n",
    "\n",
    "$$-\\log(\\mathcal{L})=-\\sum_{i=1}^{N}\\left[y_i.\\log{p(x_i)}+(1-y_i)\\log{(1-p(x_i))}\\right]$$\n",
    "\n",
    "Para ello debemos calcular el valor de la derivada del $-log(\\mathcal{L})$. Se puede demostrar que:\n",
    "\n",
    "$$\\frac{\\partial{log(\\mathcal{L})}}{\\partial{\\beta_j}}=\\sum_{i=1}^{N}(y_i-P(x_i;\\beta_1,\\beta_2,\\beta_0)).x_{ij}=\\sum_{i=1}^{N}\\left(y_i-\\frac{1}{1 + e^{-(\\beta_0+\\beta_1*x_1+\\beta_2*x_2)}}\\right).x_{ij}$$\n",
    "\n",
    "Tal como en el caso del error cuadrático medio, se puede maximizar el likelihood evaluándolo para todos los datos (Batch GD), para algunos datos (MiniBatch GS) o para un solo dato (Stochastic GD)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sklearn implementa la regresión logística. A continuación implementaremos un ejemplo para nuestros datos anteriores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "clf=linear_model.LogisticRegression(tol=0.00000004)\n",
    "clf.fit(x_train,y_train)\n",
    "plot_boundaries(x_train,x_test,y_train,y_test,clf.score(x_test,y_test),clf.predict_proba)\n",
    "plt.show()\n",
    "print(\"El umbral que se usó para dividir los patrones es: x2={}*x1+{}\".format(-x1/x2,-x0/x2))\n",
    "print(\"El umbral estimado por la regresión logística es: x2={}*x1+{}\".format(-clf.coef_[0,0]/clf.coef_[0,1],-clf.intercept_[0]/clf.coef_[0,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Regresión logística multiclase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Hasta ahora hemos clasificado los patrones de entrada en dos clases.\n",
    "¿Cómo se puede adaptar lo visto anteriormente en el caso de que $y_i$ pueda pertenecer a K clases?\n",
    "Para ello se define un estimador de probabilidad para cada una de las categorías.\n",
    "La probabilidad de que el patrón de entrada $x^i$ pertenezca a una categoría k es:\n",
    "\n",
    "$$P(Y=k|x_i,\\beta)=\\frac{e^{\\beta^k.x_i}}{\\sum_{j=1}^{K}{e^{\\beta^j.x_i}}}$$\n",
    "\n",
    "A esta función se la denomina función softmax. Esta ecuación proporciona una transición suave en cuanto a la probabilidad de pertenecer a cada clase.\n",
    "Nótese que para el caso de K=2, queda la función sigmoidea de parámetro $\\beta = \\beta^2-\\beta^1$\n",
    "\n",
    "El procedimiento para encontrar los parámetros $\\beta$ es el mismo que para la función logística. Se calcula el likelihood del modelo para un grupo de observaciones y se buscan los parámetros que maximizan dicho likelihood, es decir, minimizan la cross-entropy. Para el cálculo del gradiente de log-likelihood se puede consultar el siguiente link:\n",
    "\n",
    "http://ufldl.stanford.edu/tutorial/supervised/SoftmaxRegression/\n",
    "\n",
    "Volvamos al ejemplo de la clasificación de artículos visto anteriormente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "twenty_train = fetch_20newsgroups(subset='train', shuffle=True)\n",
    "twenty_test = fetch_20newsgroups(subset='test', shuffle=True)\n",
    "with open ('art_filt.txt', 'rb') as fp:\n",
    "    itemlist = pickle.load(fp)\n",
    "with open ('art_filt_test.txt', 'rb') as fp:\n",
    "    itemlist_test = pickle.load(fp)\n",
    "count_vect = CountVectorizer(max_df=0.9,min_df=1)\n",
    "count_vect.fit(itemlist) #Aprende el vocabulario y le asigna un código a cada palabra\n",
    "X_train_data=count_vect.transform(itemlist)\n",
    "X_test_data=count_vect.transform(itemlist_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   28.4s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   28.4s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=0.01, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='multinomial',\n",
       "          n_jobs=1, penalty='l2', random_state=None, solver='lbfgs',\n",
       "          tol=0.0001, verbose=5, warm_start=False)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "clf=linear_model.LogisticRegression(multi_class='multinomial',solver='lbfgs',max_iter=100,verbose=5,C=0.01)\n",
    "clf.fit(X_train_data,twenty_train[\"target\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.94564256673148317"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(X_train_data,twenty_train[\"target\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.77602230483271373"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(X_test_data,twenty_test[\"target\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio: Responder las siguientes preguntas\n",
    "\n",
    "- ¿Qué debo modificar si el umbral que quiero definir no es una recta?  \n",
    "- ¿Puedo entrenar un regresor logístico para que se comporte como una compuerta AND con un accuracy del 100%? En caso de que se pueda, entrene un regresor logístico para que se comporte como una compuerta AND de 2 entradas. Caso contrario, justifique por qué no se puede.\n",
    "- ¿Puedo entrenar un regresor logístico para que se comporte como una compuerta XOR con un accuracy del 100%?  En caso de que se pueda, entrene un regresor logístico para que se comporte como una compuerta XOR de 2 entradas. Caso contrario, justifique por qué no se puede."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (weather)",
   "language": "python",
   "name": "weather"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "12px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
